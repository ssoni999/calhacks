# Data Engineer - Big Data & Analytics | Assessment Rubric

## Overview
This rubric assesses candidates for Data Engineer positions focused on building scalable data pipelines, ETL processes, and data infrastructure. Strong emphasis on Spark, SQL, and cloud data platforms.

## Scoring Categories & Weights

| Category | Weight | Max Score |
|----------|--------|-----------|
| Big Data & Processing | 35% | 100 |
| Data Engineering Skills | 30% | 100 |
| Cloud Data Platforms | 20% | 100 |
| Experience & Impact | 10% | 100 |
| Education & Domain | 5% | 100 |

**Overall Score** = (Big Data × 0.35) + (DE Skills × 0.30) + (Cloud × 0.20) + (Experience × 0.10) + (Education × 0.05)

---

## 1. Big Data & Processing Score (Weight: 35%)

### Apache Spark (40 points)
**Required:** PySpark or Scala Spark, distributed processing

**Scoring:**
- **90-100 pts:** Spark expert, optimized jobs, advanced transformations, performance tuning
- **80-89 pts:** Strong Spark skills, built production data pipelines
- **60-79 pts:** Working Spark knowledge, created basic pipelines
- **40-59 pts:** Limited Spark exposure
- **0-39 pts:** No Spark experience

### Distributed Data Processing (30 points)
**Technologies:** Hadoop, MapReduce, distributed computing concepts

**Scoring:**
- **90-100 pts:** Deep understanding of distributed processing, Hadoop ecosystem, optimization
- **80-89 pts:** Strong distributed processing knowledge, worked with big data
- **60-79 pts:** Basic distributed processing understanding
- **40-59 pts:** Limited big data experience
- **0-39 pts:** No distributed processing knowledge

### Stream Processing (30 points)
**Technologies:** Kafka, Spark Streaming, Flink, real-time processing

**Scoring:**
- **90-100 pts:** Expert in stream processing, built real-time pipelines, event-driven architecture
- **80-89 pts:** Strong streaming experience, Kafka or similar
- **60-79 pts:** Basic streaming knowledge, worked with message queues
- **40-59 pts:** Limited real-time processing
- **0-39 pts:** Batch processing only

**Keywords to Look For:**
- Spark: Apache Spark, PySpark, Spark SQL, DataFrame, RDD, optimization
- Hadoop: Hadoop, HDFS, MapReduce, Hive, Pig, distributed processing
- Streaming: Kafka, Spark Streaming, Flink, real-time, streaming, event-driven
- Big Data: "processing TB/PB", "distributed", "parallel processing", "at scale"

---

## 2. Data Engineering Skills Score (Weight: 30%)

### SQL & Query Optimization (35 points)
**Required:** Advanced SQL, query optimization, database design

**Scoring:**
- **90-100 pts:** SQL expert, complex queries, optimization, window functions, CTEs
- **80-89 pts:** Strong SQL skills, query tuning, good database knowledge
- **60-79 pts:** Good SQL fundamentals, can write complex queries
- **40-59 pts:** Basic SQL knowledge
- **0-39 pts:** Limited SQL skills

### Programming Skills (35 points)
**Required:** Python (preferred) or Scala, data manipulation

**Scoring:**
- **90-100 pts:** Expert Python/Scala, pandas, numpy, data structures, algorithms
- **80-89 pts:** Strong programming skills, comfortable with data manipulation
- **60-79 pts:** Good programming fundamentals for data tasks
- **40-59 pts:** Basic programming
- **0-39 pts:** Limited programming skills

### Workflow Orchestration (30 points)
**Technologies:** Airflow, Luigi, Prefect, workflow management

**Scoring:**
- **90-100 pts:** Expert with Airflow or similar, complex DAGs, scheduling, monitoring
- **80-89 pts:** Strong orchestration experience, managed data workflows
- **60-79 pts:** Worked with workflow tools, basic DAGs
- **40-59 pts:** Limited orchestration experience
- **0-39 pts:** No workflow management

**Keywords to Look For:**
- SQL: SQL, queries, optimization, indexing, joins, window functions, CTEs
- Python: Python, pandas, numpy, data manipulation, PySpark
- Scala: Scala, functional programming (for Spark)
- Orchestration: Airflow, Luigi, Prefect, DAG, scheduling, workflow
- ETL: "ETL", "ELT", "data pipeline", "data transformation"

---

## 3. Cloud Data Platforms Score (Weight: 20%)

### Data Warehouses (50 points)
**Required:** Snowflake, Redshift, BigQuery, or similar

**Scoring:**
- **90-100 pts:** Expert with modern data warehouses, optimization, data modeling
- **80-89 pts:** Strong experience with cloud data warehouses
- **60-79 pts:** Worked with data warehouses, basic knowledge
- **40-59 pts:** Limited warehouse experience
- **0-39 pts:** No data warehouse experience

### Cloud Data Services (50 points)
**Technologies:** AWS (S3, EMR, Glue, Athena), GCP data services

**Scoring:**
- **90-100 pts:** Extensive cloud data platform experience, multiple services, architecture
- **80-89 pts:** Strong cloud data engineering, used key services
- **60-79 pts:** Working knowledge of cloud data platforms
- **40-59 pts:** Basic cloud data experience
- **0-39 pts:** No cloud data platform experience

**Keywords to Look For:**
- Warehouses: Snowflake, Redshift, BigQuery, data warehouse, columnar storage
- AWS Data: S3, EMR, Glue, Athena, Redshift, Kinesis
- GCP Data: BigQuery, Dataflow, Pub/Sub, Cloud Storage
- Data Lake: data lake, data lakehouse, Delta Lake, Iceberg
- Storage: Parquet, ORC, Avro, columnar, compression

---

## 4. Experience & Impact Score (Weight: 10%)

### Years of Data Engineering (30 points)
- **90-100 pts:** 6+ years of data engineering experience
- **80-89 pts:** 4-5 years of data engineering experience
- **60-79 pts:** 2-3 years of data engineering experience
- **40-59 pts:** 1-2 years or transitioning to data engineering
- **0-39 pts:** Less than 1 year

### Scale & Complexity (40 points)
- **90-100 pts:** Processed terabytes/petabytes, complex data architectures
- **80-89 pts:** Large-scale data processing, production pipelines
- **60-79 pts:** Moderate data volumes, production experience
- **40-59 pts:** Small-scale data processing
- **0-39 pts:** Limited production data work

### Data Impact (30 points)
- **90-100 pts:** Significant improvements (processing time, costs, data quality)
- **80-89 pts:** Measurable impact on data infrastructure
- **60-79 pts:** Contributed to data improvements
- **40-59 pts:** Some impact mentioned
- **0-39 pts:** No impact metrics

**Keywords to Look For:**
- Experience: "Data Engineer", years of experience, dates
- Scale: "TB", "PB", "millions of records", "billions of events", "processing X data daily"
- Impact: "reduced processing time by X%", "improved data quality", "cost savings"
- Production: "production pipelines", "production data", deployed, maintained

---

## 5. Education & Domain Score (Weight: 5%)

### Formal Education (60 points)
- **90-100 pts:** Master's/PhD in Data Science, CS, or Statistics from top institution
- **80-89 pts:** Master's in Data Science, CS, or quantitative field
- **60-79 pts:** Bachelor's in CS, Math, Engineering, or data-related field
- **40-59 pts:** Bachelor's in any technical field
- **0-39 pts:** Non-technical degree or self-taught

### Data Domain Knowledge (40 points)
- **90-100 pts:** Deep data domain expertise, data modeling, data governance, quality
- **80-89 pts:** Strong data fundamentals, modeling, quality awareness
- **60-79 pts:** Good data knowledge, basic modeling
- **40-59 pts:** Limited data domain knowledge
- **0-39 pts:** Minimal data understanding

**Keywords to Look For:**
- Education: Master's, MS, PhD, Data Science, Statistics, Computer Science, MIT, Stanford
- Domain: data modeling, dimensional modeling, data quality, data governance
- Concepts: star schema, snowflake schema, slowly changing dimensions, data lineage
- Quality: data validation, data testing, Great Expectations, dbt

---

## Scoring Interpretation

### Overall Score Ranges

| Score Range | Assessment | Action |
|-------------|------------|--------|
| 90-100 | Exceptional Data Engineer | Strong Hire |
| 80-89 | Strong Data Engineer | Hire |
| 70-79 | Good Data Engineer | Consider |
| 60-69 | Moderate | Maybe - Verify technical depth |
| 50-59 | Below Bar | Likely No |
| 0-49 | Not Qualified | No |

---

## Must-Have Requirements

These are critical for data engineering roles:
- **SQL proficiency** (non-negotiable)
- **Python or Scala** programming skills
- **Spark experience** (or similar big data technology)
- **Data pipeline experience** (ETL/ELT)
- **Cloud data platform** knowledge

**Missing any should cap score at 65 max**

---

## Red Flags (Automatic Deductions)

- **No Spark or big data experience (-25 pts)**
- **Weak SQL skills (-25 pts)**
- **No Python/Scala programming (-20 pts)**
- **No cloud platform experience (-20 pts)**
- **No data warehouse experience (-15 pts)**
- **No production data pipeline experience (-15 pts)**
- **No workflow orchestration (-10 pts)**
- **Backend engineer with no data experience (-20 pts)**

---

## Bonus Points

- **Master's in Data Science or Statistics (+10 pts)**
- **Real-time streaming expertise (+10 pts)**
- **Multiple data warehouses experience (+10 pts)**
- **dbt or data quality tools (+10 pts)**
- **ML pipeline experience (+10 pts)**
- **Data governance/lineage tools (+5 pts)**
- **Published data engineering content (+5 pts)**
- **Contributions to Spark/data tools (+10 pts)**

---

## Example Calculation

**Data Engineer Candidate Example:**
- Big Data & Processing: 88 pts × 0.35 = 30.8
- Data Engineering Skills: 85 pts × 0.30 = 25.5
- Cloud Data Platforms: 90 pts × 0.20 = 18.0
- Experience & Impact: 82 pts × 0.10 = 8.2
- Education: 85 pts × 0.05 = 4.25

**Overall Score: 86.75** → **Strong Data Engineer - Hire**

---

## Technical Interview Focus Areas

Based on score:
- **90-100:** Complex pipeline design, Spark optimization, data architecture
- **80-89:** ETL design, SQL challenges, Spark questions, data modeling
- **70-79:** Core skills verification, SQL test, basic Spark, pipeline design
- **60-69:** Fundamentals check, SQL proficiency, Python/data manipulation

---

## Additional Assessment Dimensions

### Data Quality Focus (Bonus Assessment)
- Data validation and testing practices
- Data quality monitoring
- Handling bad data and edge cases
- Schema evolution strategies

### ML/Analytics Support (Bonus Assessment)
- Feature engineering experience
- ML pipeline support
- Collaboration with data scientists
- Feature store implementation

### Performance & Optimization (Bonus Assessment)
- Spark job optimization
- Query performance tuning
- Cost optimization strategies
- Data partitioning strategies

---

## Notes for Reviewers

1. **SQL is fundamental** - Weak SQL is a dealbreaker
2. **Spark is highly important** - Should have real production Spark experience
3. **Look for scale indicators** - TB/PB processed, not just MB/GB
4. **Data quality matters** - Do they mention testing, validation, monitoring?
5. **Cloud experience is expected** - On-prem only is a concern
6. **Programming depth** - Should be comfortable coding, not just SQL
7. **ETL vs ELT understanding** - Should know the difference and trade-offs
8. **Data modeling knowledge** - Understanding of dimensional modeling
9. **Orchestration is key** - Airflow or similar workflow management
10. **Real-time vs batch** - Understanding of both paradigms

---

## Data Engineering vs Analytics Engineer

If candidate seems more analytics-focused:
- **Data Engineer:** Pipeline building, Spark, infrastructure, large-scale processing
- **Analytics Engineer:** dbt, SQL-heavy, BI tools, data modeling for analytics

Make sure candidate aligns with data engineering focus (infrastructure & pipelines).

---

## Domain-Specific Considerations

Some candidates may have domain expertise that's valuable:
- **FinTech:** Financial data, compliance, real-time transactions
- **AdTech:** High-volume events, attribution, real-time bidding
- **Healthcare:** HIPAA compliance, patient data, regulatory
- **E-commerce:** User behavior, recommendation systems, inventory

Consider domain fit when scoring is borderline.

